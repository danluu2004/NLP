# -*- coding: utf-8 -*-
"""nlpvit5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JA35rwPushjZV_EOTeQo28FKw4CK73_7
"""

!pip -q install transformers==4.55.2 datasets sentencepiece evaluate sacrebleu

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/drive')
!unzip "/content/drive/MyDrive/ViLexNorm-main.zip" -d /content/ViLexNorm
# %cd /content/ViLexNorm/ViLexNorm-main

import os
import random
import numpy as np
import torch


from datasets import load_dataset, DatasetDict
from transformers import (
  AutoTokenizer,
  AutoModelForSeq2SeqLM,
  DataCollatorForSeq2Seq,
  Seq2SeqTrainer,
  Seq2SeqTrainingArguments,
  set_seed,
)
import evaluate


# Reproducibility
RANDOM_SEED = 42
set_seed(RANDOM_SEED)
random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)
torch.manual_seed(RANDOM_SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(RANDOM_SEED)


DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", DEVICE)

data_dir = "/content/ViLexNorm/ViLexNorm-main/data"

raw_datasets = load_dataset(
    "csv",
    data_files={
        "train": f"{data_dir}/train.csv",
        "validation": f"{data_dir}/dev.csv",
        "test": f"{data_dir}/test.csv",
    }
)

print(raw_datasets)
print(raw_datasets["train"][0])

MODEL_NAME = "VietAI/vit5-base"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(DEVICE)

max_input_length = 128
max_target_length = 128

def preprocess_function(examples):
    inputs = examples["original"]  # ƒë·ªïi n·∫øu t√™n c·ªôt kh√°c
    targets = examples["normalized"]
    model_inputs = tokenizer(
        inputs,
        max_length=max_input_length,
        truncation=True,
        padding="max_length",
    )

    with tokenizer.as_target_tokenizer():
        labels = tokenizer(
            targets,
            max_length=max_target_length,
            truncation=True,
            padding="max_length",
        )

    label_ids = labels["input_ids"]
    label_ids = [[(l if l != tokenizer.pad_token_id else -100) for l in label] for label in label_ids]
    model_inputs["labels"] = label_ids
    return model_inputs

# Apply
processed_datasets = raw_datasets.map(preprocess_function, batched=True)
print(processed_datasets["train"][0])

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, label_pad_token_id=-100)
rouge = evaluate.load("rouge")

def postprocess_text(preds, labels):
    preds = [pred.strip() for pred in preds]
    labels = [label.strip() for label in labels]
    return preds, labels

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)
    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)
    result = {k: round(v * 100, 4) for k, v in result.items()}
    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]
    result["gen_len"] = np.mean(prediction_lens)
    return result

output_dir = "./vit5-vilexnorm"

training_args = Seq2SeqTrainingArguments(
    output_dir=output_dir,
    report_to="none",   # t·∫Øt wandb
    eval_strategy="epoch",
    save_strategy="epoch",
    learning_rate=5e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=5,
    weight_decay=0.01,
    save_total_limit=2,
    predict_with_generate=True,
    fp16=torch.cuda.is_available(),
    logging_steps=50,
)

# %%
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=processed_datasets["train"],
    eval_dataset=processed_datasets["validation"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

# Train
trainer.train()
# trainer.save_model(output_dir)

def generate(texts, max_length=128, num_beams=4):
    model.eval()
    inputs = tokenizer(texts, return_tensors="pt", padding=True, truncation=True, max_length=max_input_length).to(DEVICE)
    outputs = model.generate(**inputs, max_length=max_length, num_beams=num_beams, early_stopping=True)
    return tokenizer.batch_decode(outputs, skip_special_tokens=True)

examples = ["sv ƒëh gia ƒë√¨nh ch∆∞a cho ƒëi l√†m"]
print("Inference:", generate(examples))

# Save
model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)

# Reload
# tokenizer = AutoTokenizer.from_pretrained(output_dir)
# model = AutoModelForSeq2SeqLM.from_pretrained(output_dir).to(DEVICE)

# %% [markdown]
# ## 8) Next steps
# - Ki·ªÉm tra ch√≠nh x√°c t√™n c·ªôt trong train/dev/test.csv r·ªìi ch·ªânh l·∫°i `preprocess_function`.
# - TƒÉng batch size / epochs theo GPU.
# - ƒê√°nh gi√° b·∫±ng BLEU, chrF ngo√†i ROUGE n·∫øu c·∫ßn.
# - Deploy API b·∫±ng FastAPI ho·∫∑c Streamlit sau khi fine-tune xong.

import pandas as pd
import matplotlib.pyplot as plt

# ƒê√°nh gi√° b·∫±ng trainer
metrics = trainer.evaluate(processed_datasets["test"])

# D·ª± ƒëo√°n chi ti·∫øt
preds, labels, _ = trainer.predict(processed_datasets["test"])
decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)
labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

# Post-process
decoded_preds = [p.strip() for p in decoded_preds]
decoded_labels = [l.strip() for l in decoded_labels]

# ROUGE
rouge_scores = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)
rouge_scores = {k: round(v * 100, 2) for k, v in rouge_scores.items()}

# BLEU
import evaluate
bleu = evaluate.load("bleu")
bleu_score = bleu.compute(predictions=decoded_preds, references=[[l] for l in decoded_labels])
bleu_score = round(bleu_score["bleu"] * 100, 2)

# L·∫•y gen_len n·∫øu c√≥
gen_len = round(metrics.get("eval_gen_len", np.mean([len(p.split()) for p in decoded_preds])), 2)

# G·ªôp k·∫øt qu·∫£
results = {
    "Test Loss": round(metrics["eval_loss"], 4),
    "ROUGE-1": rouge_scores["rouge1"],
    "ROUGE-2": rouge_scores["rouge2"],
    "ROUGE-L": rouge_scores["rougeL"],
    "ROUGE-Lsum": rouge_scores["rougeLsum"],
    "BLEU": bleu_score,
    "Avg Gen Len": gen_len
}

# Xu·∫•t b·∫£ng ƒë·∫πp
df_results = pd.DataFrame([results])
print("üìä Evaluation Results")
display(df_results)

# V·∫Ω bi·ªÉu ƒë·ªì c·ªôt
plt.figure(figsize=(8,5))
metric_names = ["ROUGE-1", "ROUGE-2", "ROUGE-L", "ROUGE-Lsum", "BLEU"]
values = [results[m] for m in metric_names]

plt.bar(metric_names, values)
plt.title("Model Evaluation Metrics on Test Set")
plt.ylabel("Score (%)")
plt.ylim(0, 100)
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

def plot_training_curves(trainer):
    """
    V·∫Ω bi·ªÉu ƒë·ªì qu√° tr√¨nh hu·∫•n luy·ªán t·ª´ trainer.state.log_history
    H·ªó tr·ª£ c·∫£ Loss v√† c√°c ch·ªâ s·ªë eval (ROUGE, BLEU, v.v.)
    """
    logs = pd.DataFrame(trainer.state.log_history)
    if logs.empty:
        print("‚ö†Ô∏è Kh√¥ng c√≥ log n√†o trong trainer.state.log_history.")
        return

    # ----- V·∫Ω Loss -----
    plt.figure(figsize=(8,5))
    if "loss" in logs:
        plt.plot(logs["step"], logs["loss"], label="Training Loss", color="blue")
    if "eval_loss" in logs:
        plt.plot(logs["step"], logs["eval_loss"], label="Validation Loss", color="orange")
    plt.xlabel("Steps")
    plt.ylabel("Loss")
    plt.title("Training & Validation Loss")
    plt.legend()
    plt.grid(True, linestyle="--", alpha=0.6)
    plt.show()

    # ----- V·∫Ω c√°c metrics kh√°c -----
    metrics_cols = [c for c in logs.columns if c.startswith("eval_") and c not in ["eval_loss"]]
    for m in metrics_cols:
        plt.figure(figsize=(8,5))
        plt.plot(logs["step"], logs[m], marker="o", color="green")
        plt.xlabel("Steps")
        plt.ylabel(m)
        plt.title(f"Evaluation {m} over training")
        plt.grid(True, linestyle="--", alpha=0.6)
        plt.show()
        plt.tight_layout()
        plt.show(block=True)  # √©p matplotlib render

# Ho·∫∑c l∆∞u file ra r·ªìi m·ªü
plt.savefig("training_loss.png")
print("‚úÖ Bi·ªÉu ƒë·ªì ƒë√£ l∆∞u t·∫°i training_loss.png")
